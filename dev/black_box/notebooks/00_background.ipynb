{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import rdkit.Chem.Descriptors\n",
    "from IPython.display import Image, display, SVG, HTML\n",
    "from rdkit.Chem import PandasTools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric\n",
    "from IPython.display import Markdown as md\n",
    "import dill as pickle\n",
    "import pandas as ps\n",
    "PandasTools.RenderImagesInAllDataFrames(images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"../../..\" not in sys.path:\n",
    "    sys.path.append(\"../../..\")\n",
    "import molNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DPI=300\n",
    "DEFAULT_IMG_PLOT_WIDTH=400\n",
    "SEED=2\n",
    "TEST_SMILES='COc1ccccc1[N+](=O)[O-]'\n",
    "DEFAULT_PATIENCE=10\n",
    "\n",
    "REDRAW=False\n",
    "REMODEL=False\n",
    "\n",
    "\n",
    "if REMODEL:\n",
    "    REDRAW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SMILES=Chem.MolToSmiles(Chem.MolFromSmiles(TEST_SMILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_pred(model,loader,target_file=None):\n",
    "    true=[]\n",
    "    pred=[]\n",
    "    try:\n",
    "        loader.test_dataloader()\n",
    "    except:\n",
    "        loader.setup()\n",
    "    for i,d in enumerate(loader.test_dataloader()):\n",
    "        pred.extend(model(d.to(model.device)).detach().cpu().numpy().flatten())\n",
    "        true.extend(d.y.detach().cpu().numpy().flatten())\n",
    "        \n",
    "        \n",
    "    plt.plot(true,pred,\"o\")\n",
    "    if target_file is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(target_file),exist_ok=True)\n",
    "        plt.savefig(target_file,dpi=DEFAULT_DPI)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_validation(model,loader,categories,target_file=None, ignore_empty=True):\n",
    "    true=[]\n",
    "    pred_correct=[]\n",
    "    pred_wrong=[]\n",
    "    for i,d in enumerate(loader.test_dataloader()):\n",
    "        pred_m=model(d.to(model.device)).detach().cpu().numpy()\n",
    "        p=pred_m.argmax(1)\n",
    "        t=d.y.detach().cpu().numpy().argmax(1)\n",
    "        pred_correct.extend(p[p==t])\n",
    "        pred_wrong.extend(p[p!=t])\n",
    "        true.extend(t)\n",
    "        \n",
    "        \n",
    "    #plt.hist(true)\n",
    "    #plt.hist(pred)\n",
    "    categories = np.array(categories)\n",
    "    \n",
    "   \n",
    "    labels_true, counts_true = np.unique(true, return_counts=True)\n",
    "    labels_pred_correct, counts_pred_correct = np.unique(pred_correct, return_counts=True)\n",
    "    labels_pred_wrong, counts_pred_wrong = np.unique(pred_wrong, return_counts=True)\n",
    "    \n",
    "    if ignore_empty:\n",
    "        all_labels=np.array(list(set(labels_true) | set(labels_pred_correct)| set(labels_pred_wrong)))\n",
    "        label_list=all_labels.tolist()\n",
    "        labels_true=np.array([label_list.index(l) for l in labels_true])\n",
    "        labels_pred_correct=np.array([label_list.index(l) for l in labels_pred_correct])\n",
    "        labels_pred_wrong=np.array([label_list.index(l) for l in labels_pred_wrong])\n",
    "    \n",
    "    plt.bar(labels_true-0.2, counts_true, align='center',width=0.2,label=\"true\")\n",
    "    plt.bar(labels_pred_correct, counts_pred_correct, align='center',width=0.2,label=\"correct predicted\")\n",
    "    plt.bar(labels_pred_wrong+0.2, counts_pred_wrong, align='center',width=0.2,label=\"wrong predicted\")\n",
    "    \n",
    "    #n, bins, patches = plt.hist([true,pred], len(categories), density=False)\n",
    "    #print(bins, len(categories))\n",
    "    x=np.arange(len(categories))\n",
    "    if ignore_empty:\n",
    "        x=np.arange(len(all_labels))\n",
    "        categories = categories[all_labels]\n",
    "    #print(x,categories)\n",
    "    plt.xticks(ticks=x,labels=categories, rotation=90, horizontalalignment='left')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if target_file is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        os.makedirs(os.path.dirname(target_file),exist_ok=True)\n",
    "        plt.savefig(target_file,dpi=DEFAULT_DPI)\n",
    "    plt.close()\n",
    "\n",
    "#plot_category_validation(model,loader,atom_hybridization_one_hot.describe_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "class ClearCallback(pl.Callback):\n",
    "    def on_validation_epoch_end(self,*args,**kwargs):\n",
    "        self.clear()\n",
    "    \n",
    "    def clear(self):\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "class StoreMetricsCallback(pl.Callback):\n",
    "    def __init__(self,live_plot=True,final_save=None,plot_ignore_first=True,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.data={}\n",
    "        self.live_plot=live_plot\n",
    "        self.final_save=final_save\n",
    "        self.plot_ignore_first=plot_ignore_first\n",
    "    \n",
    "    def plot_data(self,save=None):\n",
    "        plt.figure()\n",
    "        for label,data in self.data.items():\n",
    "            if self.plot_ignore_first and len(data[0])>1:\n",
    "                plt.plot(data[0][1:],data[1][1:], label=label)\n",
    "            else:\n",
    "                plt.plot(data[0],data[1], label=label)\n",
    "        plt.legend()\n",
    "        if save:\n",
    "            plt.savefig(save,dpi=DEFAULT_DPI)\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def on_validation_epoch_end(self,trainer, pl_module,*args,**kwargs):\n",
    "        ep=trainer.current_epoch\n",
    "        for k,v in trainer.callback_metrics.items():\n",
    "            if k not in self.data:\n",
    "                self.data[k]=([],[])\n",
    "            self.data[k][0].append(ep)\n",
    "            self.data[k][1].append(v.detach().cpu().numpy())\n",
    "        if self.live_plot:\n",
    "            self.plot_data()\n",
    "        \n",
    "        #if 'val_loss' in self.data:\n",
    "        #    display(','.join([str(i) for i in self.data[\"val_loss\"][1]]))\n",
    "            \n",
    "\n",
    "    #on_validation_epoch_end = on_epoch_end\n",
    "    #on_test_epoch_end = on_epoch_end\n",
    "    #on_train_epoch_end = on_epoch_end\n",
    "    \n",
    "    def on_train_end(self,trainer, pl_module):\n",
    "        self.plot_data(self.final_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import dill as pickle\n",
    "\n",
    "def default_model_run(model_name,model,loader,force_run=False,detect_lr=True,show_tb=True,\n",
    "                      train=True,save=True,test=True,live_plot=True,max_epochs=1000,min_epochs=1,\n",
    "                      ignore_load_error=False,force_test_data_reload=False,early_stopping=True,early_stopping_delta=10**(-6),\n",
    "                      categories=None,plot_ignore_first=True,early_stop_patience=DEFAULT_PATIENCE,\n",
    "                     ):\n",
    "    \n",
    "    data={\"files\":{}}\n",
    "    data[\"model_name\"]=model_name\n",
    "    data[\"files\"][\"model_dir\"]=os.path.join(\"models\",data[\"model_name\"])\n",
    "\n",
    "    data[\"files\"][\"logdir\"]=os.path.join(data[\"files\"][\"model_dir\"],\"logs\")\n",
    "    data[\"files\"][\"tb_logdir\"]=os.path.join(data[\"files\"][\"logdir\"],\"tensorboard\")\n",
    "    \n",
    "    data[\"files\"][\"plot_dir\"]=os.path.join(data[\"files\"][\"model_dir\"],\"plots\")\n",
    "    \n",
    "    os.makedirs(data[\"files\"][\"plot_dir\"],exist_ok=True)\n",
    "    data[\"files\"][\"true_pred_plt\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"tvp.png\")\n",
    "    data[\"files\"][\"lr_optim_plot\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"lrp.png\")\n",
    "    data[\"files\"][\"metrics_plot\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"metrics.png\")\n",
    "    data[\"files\"][\"cat_plot\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"cat_validation.png\")\n",
    "    data[\"files\"][\"model_plot\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"model_plot.png\")\n",
    "    data[\"files\"][\"model_plot_img\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"model_plot_img.png\")\n",
    "    data[\"files\"][\"img_graph_plot\"]=os.path.join(data[\"files\"][\"plot_dir\"],\"img_graph_plot\")\n",
    "    os.makedirs(data[\"files\"][\"img_graph_plot\"],exist_ok=True)\n",
    "    \n",
    "    data[\"files\"][\"model_checkpoint\"]=os.path.join(data[\"files\"][\"model_dir\"],\"model.ckpt\")\n",
    "    data[\"files\"][\"test_data_file\"]=os.path.join(data[\"files\"][\"model_dir\"],\"test_data.pickle\")\n",
    "    data[\"files\"][\"test_batch_file\"]=os.path.join(data[\"files\"][\"model_dir\"],\"test_batch.pickle\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if not force_run :\n",
    "        try:\n",
    "            model = model.__class__.load_from_checkpoint(data[\"files\"][\"model_checkpoint\"],\n",
    "                                                         map_location=lambda storage, location: storage)\n",
    "        except:\n",
    "            if ignore_load_error:\n",
    "                pass\n",
    "            else:\n",
    "                force_run=True\n",
    "        try:\n",
    "            test_data = pickle.load( open( data[\"files\"][\"test_data_file\"], \"rb\" ) )\n",
    "            test_batch = pickle.load( open( data[\"files\"][\"test_batch_file\"], \"rb\" ) )\n",
    "        except:\n",
    "            force_run=True\n",
    "    \n",
    "    if force_test_data_reload or force_run:\n",
    "        test_data=find_test_data(loader)\n",
    "        test_batch=iter(torch_geometric.data.DataLoader([test_data])).next()\n",
    "        pickle.dump( test_data, open( data[\"files\"][\"test_data_file\"], \"wb\" ) )\n",
    "        pickle.dump( test_batch, open( data[\"files\"][\"test_batch_file\"], \"wb\" ) )\n",
    "        \n",
    "    if force_run:\n",
    "        \n",
    "        try:\n",
    "            loader.test_dataloader()\n",
    "        except:\n",
    "            loader.setup()\n",
    "        \n",
    "        if detect_lr:\n",
    "            lr_trainer = pl.Trainer()\n",
    "            lr_finder = lr_trainer.tuner.lr_find(model,train_dataloader=loader.train_dataloader(),max_lr=10**2)\n",
    "            fig = lr_finder.plot(suggest=True)\n",
    "            \n",
    "            plt.savefig(data[\"files\"][\"lr_optim_plot\"],dpi=DEFAULT_DPI)\n",
    "            plt.close()\n",
    "            \n",
    "            model.lr = lr_finder.suggestion()\n",
    "            print(\"set lr to\",model.lr)\n",
    "    \n",
    "        if train or test:\n",
    "            clear_cb=ClearCallback()\n",
    "            tb_logger = TensorBoardLogger(data[\"files\"][\"tb_logdir\"])\n",
    "            checkpoint_callback = ModelCheckpoint(monitor='val_loss',verbose=True)\n",
    "            metrics_cb = StoreMetricsCallback(live_plot=live_plot,\n",
    "                                 final_save=data[\"files\"][\"metrics_plot\"],\n",
    "                                              plot_ignore_first=plot_ignore_first,\n",
    "                                 )\n",
    "            early_stop_cb = EarlyStopping(\n",
    "                        monitor='val_loss',patience=early_stop_patience,\n",
    "                min_delta=early_stopping_delta,\n",
    "            )\n",
    "            \n",
    "            cb=[]\n",
    "            \n",
    "            if early_stopping:\n",
    "                cb=[early_stop_cb]\n",
    "            \n",
    "            cb.extend([\n",
    "                checkpoint_callback,\n",
    "                clear_cb,\n",
    "                metrics_cb,\n",
    "               ])\n",
    "            \n",
    "            trainer = pl.Trainer(max_epochs=max_epochs,\n",
    "                                 gpus=torch.cuda.device_count(),\n",
    "                                 callbacks=cb,\n",
    "                                 logger=tb_logger,\n",
    "                                 terminate_on_nan=True,\n",
    "                                 min_epochs=min_epochs,\n",
    "                                )\n",
    "\n",
    "        if train:\n",
    "            trainer.fit(model,loader)\n",
    "            model = model.__class__.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "            \n",
    "        if test:\n",
    "            trainer.test(model=model,ckpt_path=None)\n",
    "            \n",
    "        if save:\n",
    "            trainer.save_checkpoint(data[\"files\"][\"model_checkpoint\"])\n",
    "        \n",
    "        \n",
    "    \n",
    "    model.to('cpu')\n",
    "    if REDRAW or force_run:\n",
    "        plot_true_pred(model,loader,target_file=data[\"files\"][\"true_pred_plt\"])\n",
    "        if categories:\n",
    "            plot_category_validation(model,loader,categories,target_file=data[\"files\"][\"cat_plot\"])\n",
    "    \n",
    "    if hasattr(model,\"to_graphviz_from_batch\"):\n",
    "        g=model.to_graphviz_from_batch(test_batch,reduced=True)\n",
    "        g.format='png'           \n",
    "        #g.engine=\"fdp\"\n",
    "        g.render(filename=os.path.basename(data[\"files\"][\"model_plot\"]).replace(\".\"+g.format,\"\"),\n",
    "                       directory=os.path.dirname(data[\"files\"][\"model_plot\"]))\n",
    "    \n",
    "    if hasattr(model,\"to_graphviz_images_from_batch\"):\n",
    "        g=model.to_graphviz_images_from_batch(test_batch,\n",
    "                                       path=os.path.abspath(data[\"files\"][\"img_graph_plot\"],),\n",
    "                                      )\n",
    "        g.format='png'           \n",
    "        #g.engine=\"fdp\"\n",
    "        g.render(filename=os.path.basename(data[\"files\"][\"model_plot_img\"]).replace(\".\"+g.format,\"\"),\n",
    "                       directory=os.path.dirname(data[\"files\"][\"model_plot\"]))\n",
    "    \n",
    "    if os.path.exists(data[\"files\"][\"lr_optim_plot\"]):\n",
    "        display(Image(data[\"files\"][\"lr_optim_plot\"], width=DEFAULT_IMG_PLOT_WIDTH))\n",
    "    if os.path.exists(data[\"files\"][\"metrics_plot\"]):\n",
    "        display(Image(data[\"files\"][\"metrics_plot\"], width=DEFAULT_IMG_PLOT_WIDTH))\n",
    "    if os.path.exists(data[\"files\"][\"true_pred_plt\"]):\n",
    "        display(Image(data[\"files\"][\"true_pred_plt\"], width=DEFAULT_IMG_PLOT_WIDTH))\n",
    "    if os.path.exists(data[\"files\"][\"cat_plot\"]):\n",
    "        display(Image(data[\"files\"][\"cat_plot\"], width=DEFAULT_IMG_PLOT_WIDTH))\n",
    "    \n",
    "    if os.path.exists(data[\"files\"][\"model_plot\"]):\n",
    "        display(Image(data[\"files\"][\"model_plot\"], width=DEFAULT_IMG_PLOT_WIDTH))\n",
    "    if os.path.exists(data[\"files\"][\"model_plot_img\"]):\n",
    "        display(Image(data[\"files\"][\"model_plot_img\"], width=DEFAULT_IMG_PLOT_WIDTH))\n",
    "        \n",
    "    data[\"test_data\"]=test_data\n",
    "    data[\"test_batch\"]=test_batch\n",
    "         \n",
    "    try:\n",
    "        data[\"trainer\"]=trainer\n",
    "    except:\n",
    "        pass\n",
    "    return model,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_test_data(loader,smiles=None):\n",
    "    if smiles is None:\n",
    "        smiles = TEST_SMILES\n",
    "    try:\n",
    "        loader.test_dataloader()\n",
    "    except:\n",
    "        loader.setup()\n",
    "    sdt=0\n",
    "    for subloader in [loader.test_dataloader(),loader.val_dataloader(),loader.train_dataloader()]:\n",
    "        for i,d in enumerate(subloader):\n",
    "            for sd in d.to_data_list():\n",
    "                if sd.string_data_titles[0][sdt] != \"index\":\n",
    "                    sdt=sd.string_data_titles[0].index(\"smiles\")\n",
    "                if smiles == sd.string_data[0][sdt]:\n",
    "                    return sd\n",
    "    raise ValueError()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "class MidpointNormalize(mpl.colors.Normalize):\n",
    "    \"\"\"Normalise the colorbar.\"\"\"\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        mpl.colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))\n",
    "    \n",
    "def plot_fcnn(layer_sizes,weights=None, biases=False,show_bar=False,input_labels=None,weight_position=None,\n",
    "              round_weights=2,edge_width=1,save=None,show=None,hide_loose=False,cmap=plt.cm.coolwarm,nodes_cmap=plt.cm.coolwarm,\n",
    "             input_array=None,layer_norm=False,\n",
    "             ):\n",
    "    g=nx.Graph()\n",
    "    pos={}\n",
    "    w=0\n",
    "    if weights is not None:\n",
    "        for i in range(len(weights)):\n",
    "            assert weights[i].shape[0] == layer_sizes[i+1],(weights[i].shape[0], layer_sizes[i+1])\n",
    "            assert weights[i].shape[1] == layer_sizes[i],(weights[i].shape[1], layer_sizes[i])\n",
    "            \n",
    "            \n",
    "            if layer_norm:\n",
    "                weights[i]=weights[i]/np.abs(weights[i]).max()\n",
    "            weights[i]=np.round(weights[i],round_weights)\n",
    "        \n",
    "    for l,n in enumerate(layer_sizes):\n",
    "        for i in range(n):\n",
    "            node=\"{}_{}\".format(l,i)\n",
    "            node_d={\"layer\":l,\n",
    "                   \"layer_pos\":i,\n",
    "                    \"show\":True\n",
    "                   }\n",
    "            \n",
    "            if input_labels is not None:\n",
    "                if len(input_labels)>len(g):\n",
    "                    node_d[\"label\"]=input_labels[len(g)]\n",
    "            g.add_node(node,**node_d)\n",
    "            #pos[node]=(l*50,-(i-n/2)*10)\n",
    "            if l>0:\n",
    "                for j in range(layer_sizes[l-1]):\n",
    "                    pnode=\"{}_{}\".format(l-1,j)\n",
    "                    ed={\"show\":True}\n",
    "                    if weights is not None:\n",
    "                        #display(weights[l-1])\n",
    "                        #display(l,n,j,i)\n",
    "                        ed[\"w\"]=weights[l-1][i][j]\n",
    "                        if hide_loose and ed[\"w\"]==0:\n",
    "                            ed[\"show\"]=False\n",
    "                        w+=1\n",
    "                    g.add_edge(pnode,node,**ed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if hide_loose:\n",
    "        for node,nd in g.nodes(data=True):\n",
    "            if g.edges(node) == 0 or all([not g.get_edge_data(*e)[\"show\"] for e in g.edges(node)]):\n",
    "                g.nodes[node][\"show\"]=False\n",
    "            \n",
    "    nodes_kwargs={}    \n",
    "    if input_array is not None and weights is not None:\n",
    "        node_values=[input_array[:layer_sizes[0]]]\n",
    "        for i,w in enumerate(weights):\n",
    "            node_values.append(np.dot(w,node_values[i]))\n",
    "            \n",
    "        if layer_norm:\n",
    "            for i in range(len(node_values)):\n",
    "                node_values[i]=node_values[i]/np.abs(node_values[i]).max()\n",
    "        \n",
    "        for node,nd in g.nodes(data=True):\n",
    "            g.nodes[node][\"value\"]=node_values[nd[\"layer\"]][nd[\"layer_pos\"]]\n",
    "                \n",
    "        vmin = min([w.min() for w in node_values])\n",
    "        vmax= max([w.max() for w in node_values])\n",
    "        sm_nodes = plt.cm.ScalarMappable(cmap=nodes_cmap, norm=MidpointNormalize(vmin, vmax, 0.))\n",
    "        \n",
    "        nodes_kwargs[\"node_color\"]=[sm_nodes.to_rgba(nd[\"value\"]) for n,nd in g.nodes(data=True) if nd[\"show\"]]\n",
    "        \n",
    "    \n",
    "    #reposition\n",
    "    layer_pos={ln:0 for ln in range(l+1)}\n",
    "    showing_layer_size=[0]*len(layer_sizes)\n",
    "    for node,nd in g.nodes(data=True):\n",
    "        if nd[\"show\"]:\n",
    "            showing_layer_size[nd[\"layer\"]]+=1\n",
    "            \n",
    "    for node,nd in g.nodes(data=True):\n",
    "        if nd[\"show\"]:\n",
    "            l=nd[\"layer\"]\n",
    "            pos[node]=(l*50,-((layer_pos[l]-showing_layer_size[l]/2)*10))\n",
    "            layer_pos[l]+=1\n",
    "        #else:\n",
    "        #    pos[node]=(0,0)\n",
    "    \n",
    "    while showing_layer_size[-1]==0:\n",
    "        showing_layer_size.pop(-1)\n",
    "\n",
    "    \n",
    "    fs=(2*(len(showing_layer_size)+1),1+max(showing_layer_size)/3)\n",
    "\n",
    "    \n",
    "    if weights is not None:\n",
    "        vmin = min([w.min() for w in weights])\n",
    "        vmax= max([w.max() for w in weights])\n",
    "        sm_edges = plt.cm.ScalarMappable(cmap=cmap, norm=MidpointNormalize(vmin, vmax, 0.))\n",
    "    \n",
    "    fig = plt.figure(figsize=fs)\n",
    "    nodes = nx.draw_networkx_nodes(\n",
    "        g, pos,nodelist=[n for  n,nd in g.nodes(data=True) if nd[\"show\"]],\n",
    "        **nodes_kwargs\n",
    "        )\n",
    "    \n",
    "    max_end_width=0\n",
    "    if input_labels:\n",
    "        for i,(node,data) in enumerate(g.nodes(data=True)):\n",
    "            if data[\"show\"] and \"label\" in data and data[\"label\"]:\n",
    "                if data[\"layer\"]==0:\n",
    "                    x,y=pos[node]\n",
    "                    x=x-6\n",
    "                    plt.text(x,y,s=data[\"label\"], bbox=dict(facecolor='white', alpha=0.5),horizontalalignment='right',verticalalignment=\"center_baseline\")\n",
    "                elif data[\"layer\"]==len(layer_sizes)-1:\n",
    "                    x,y=pos[node]\n",
    "                    x=x+6\n",
    "                    t=plt.text(x,y,s=data[\"label\"], bbox=dict(facecolor='white', alpha=0.5),horizontalalignment='left',verticalalignment=\"center_baseline\")\n",
    "                    r = fig.canvas.get_renderer()\n",
    "                    bb = t.get_window_extent(renderer=r)\n",
    "                    max_end_width = max(max_end_width,bb.width)\n",
    "                else:\n",
    "                    x,y=pos[node]\n",
    "                    plt.text(x,y,s=data[\"label\"], bbox=dict(facecolor='white', alpha=0.5),horizontalalignment='center',verticalalignment=\"center_baseline\")\n",
    "                    \n",
    "    ed={'width':edge_width,\n",
    "       'edgelist':[(n1,n2) for n1,n2,v in g.edges(data=True) if v[\"show\"]],\n",
    "       }\n",
    "    if weights is not None:\n",
    "        ed={**ed,**dict(edge_cmap= cmap,\n",
    "                edge_color=[sm_edges.to_rgba(v[\"w\"]) for n1,n2,v in g.edges(data=True) if v[\"show\"]]\n",
    "               )\n",
    "           }\n",
    "        \n",
    "        if weight_position is not None:\n",
    "            def draw_networkx_edge_labels(edge_labels,label_pos):\n",
    "                nodes = nx.draw_networkx_edge_labels(\n",
    "                            g, pos,\n",
    "                            edge_labels=edge_labels,\n",
    "                            rotate=False,\n",
    "                            label_pos=label_pos,\n",
    "                            #norm=sm_edges,\n",
    "                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='none')\n",
    "                        )\n",
    "                \n",
    "            if isinstance(weight_position,(float,int)):\n",
    "                edge_labels={(n1,n2):v[\"w\"] for n1,n2,v in g.edges(data=True) if v[\"show\"]}\n",
    "                label_pos = 1-weight_position\n",
    "                draw_networkx_edge_labels(edge_labels,label_pos)\n",
    "            else:\n",
    "                assert len(weight_position) == len(layer_sizes)-1\n",
    "                for i,wp in enumerate(weight_position):\n",
    "                    if wp is None:\n",
    "                        continue\n",
    "                    if isinstance(wp,(float,int)):\n",
    "                        edge_labels={}\n",
    "                        for n1,n2,v in g.edges(data=True):\n",
    "                            if n1.startswith(\"{}_\".format(i)) and v[\"show\"]:\n",
    "                                edge_labels[(n1,n2)]=v[\"w\"]\n",
    "                        nodes = draw_networkx_edge_labels(\n",
    "                            edge_labels=edge_labels,\n",
    "                            label_pos=1-wp,\n",
    "                        )\n",
    "                        \n",
    "        if show_bar:\n",
    "            bbox = fig.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "            \n",
    "            plt.colorbar(sm_edges,pad = 0.1+max_end_width/(bbox.width*fig.dpi))\n",
    "            if \"node_color\" in nodes_kwargs:\n",
    "                plt.colorbar(sm_nodes,pad = 0.1+max_end_width/(bbox.width*fig.dpi))\n",
    "    \n",
    "    edges = nx.draw_networkx_edges(g, pos,**ed)\n",
    "\n",
    "    #plt.tight_layout\n",
    "    plt.axis('off')\n",
    "    if save is not None:\n",
    "        os.makedirs(os.path.dirname(save),exist_ok=True)\n",
    "        plt.savefig(save,dpi=DEFAULT_DPI)\n",
    "    if show is None:\n",
    "        if save is not None:\n",
    "            show=False\n",
    "        else:\n",
    "            show=True\n",
    "    cut = 1.15\n",
    "    xmax= max(xx for xx,yy in pos.values())+10\n",
    "    ymax= max(yy for xx,yy in pos.values())+10\n",
    "    xmin= min(xx for xx,yy in pos.values())-10\n",
    "    ymin= min(yy for xx,yy in pos.values())-10\n",
    "    plt.xlim(xmin,xmax)\n",
    "    plt.ylim(ymin,ymax)\n",
    "    #fig.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "#w=np.random.random(8)-0.4\n",
    "#w[np.abs(w)<0.3]=0\n",
    "#plot_fcnn([3,2,1],w,\n",
    "#          input_labels=[\"input_{}\".format(i) for i in range(20)]+\\\n",
    "#          [\"center_{}\".format(i) for i in range(5)]+\\\n",
    "#          [\"output_{}\".format(i) for i in range(1)],\n",
    "#          weight_position=[None,0.2],edge_width=2,\n",
    "#         hide_loose=True,\n",
    "#          show_bar=True\n",
    "#)\n",
    "\n",
    "w=np.random.random(130)-0.4\n",
    "w[np.abs(w)<0.3]=0\n",
    "\n",
    "a,b,c=25,5,1\n",
    "wab,wbc=np.random.random(a*b).reshape(b,a)-0.6,np.random.random(b*c).reshape(c,b)-0.6\n",
    "wab[np.abs(wab)<0.3]=0\n",
    "wbc[np.abs(wbc)<0.3]=0\n",
    "\n",
    "\n",
    "#display(np.dot(np.arange(a),wab))\n",
    "\n",
    "#plot_fcnn([a,b,c],[wab,wbc],\n",
    "#          input_labels=[\"input_{}\".format(i) for i in range(a)]+\\\n",
    "#         [\"center_{}\".format(i) for i in range(b)]+\\\n",
    "#          [\"output_{}\".format(i) for i in range(c)],\n",
    "#          weight_position=[None,0.2],edge_width=2,\n",
    "#         hide_loose=True,\n",
    "#          show_bar=True,\n",
    "#          nodes_cmap=plt.cm.PuOr_r,\n",
    "#          input_array=np.arange(a+1),\n",
    "#          layer_norm=True,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copied from rdkit\n",
    "import rdkit.Chem.Draw as Draw\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def customGetSimilarityMapFromWeights(mol, weights, colorMap=None, scale=-1, size=(250, 250),\n",
    "                                sigma=None, coordScale=1.5, step=0.01, colors='k', contourLines=10,\n",
    "                                alpha=0.5,vmin=None,vmax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates the similarity map for a molecule given the atomic weights.\n",
    "    Parameters:\n",
    "      mol -- the molecule of interest\n",
    "      colorMap -- the matplotlib color map scheme, default is custom PiWG color map\n",
    "      scale -- the scaling: scale < 0 -> the absolute maximum weight is used as maximum scale\n",
    "                            scale = double -> this is the maximum scale\n",
    "      size -- the size of the figure\n",
    "      sigma -- the sigma for the Gaussians\n",
    "      coordScale -- scaling factor for the coordinates\n",
    "      step -- the step for calcAtomGaussian\n",
    "      colors -- color of the contour lines\n",
    "      contourLines -- if integer number N: N contour lines are drawn\n",
    "                      if list(numbers): contour lines at these numbers are drawn\n",
    "      alpha -- the alpha blending value for the contour lines\n",
    "      kwargs -- additional arguments for drawing\n",
    "    \"\"\"\n",
    "    if mol.GetNumAtoms() < 2:\n",
    "        raise ValueError(\"too few atoms\")\n",
    "\n",
    "    fig = Draw.MolToMPL(mol, coordScale=coordScale, size=size, **kwargs)\n",
    "    if sigma is None:\n",
    "        if mol.GetNumBonds() > 0:\n",
    "            bond = mol.GetBondWithIdx(0)\n",
    "            idx1 = bond.GetBeginAtomIdx()\n",
    "            idx2 = bond.GetEndAtomIdx()\n",
    "            sigma = 0.3 * np.sqrt(\n",
    "              sum([(mol._atomPs[idx1][i] - mol._atomPs[idx2][i])**2 for i in range(2)]))\n",
    "        else:\n",
    "            sigma = 0.3 * \\\n",
    "                np.sqrt(sum([(mol._atomPs[0][i] - mol._atomPs[1][i])**2 for i in range(2)]))\n",
    "        sigma = round(sigma, 2)\n",
    "    x, y, z = Draw.calcAtomGaussians(mol, sigma, weights=weights, step=step)\n",
    "    z=z/100\n",
    "    # scaling\n",
    "    if scale <= 0.0:\n",
    "        maxScale = max(np.fabs(np.min(z)), np.fabs(np.max(z)))\n",
    "    else:\n",
    "        maxScale = scale\n",
    "    # coloring\n",
    "    if colorMap is None:\n",
    "        if cm is None:\n",
    "            raise RuntimeError(\"matplotlib failed to import\")\n",
    "        PiYG_cmap = cm.get_cmap('PiYG', 2)\n",
    "        colorMap = LinearSegmentedColormap.from_list(\n",
    "            'PiWG', [PiYG_cmap(0), (1.0, 1.0, 1.0), PiYG_cmap(1)], N=255)\n",
    "    \n",
    "    if vmin is None:\n",
    "        vmin=-maxScale\n",
    "    if vmax is None:\n",
    "        vmax=maxScale\n",
    "    sm_nodes = plt.cm.ScalarMappable(cmap=colorMap, norm=MidpointNormalize(vmin, vmax, 0.))\n",
    "    \n",
    "    z+=1e-6\n",
    "    \n",
    "    a = fig.axes[0].imshow(z, cmap=colorMap, interpolation='bilinear', origin='lower',\n",
    "                       extent=(0, 1, 0, 1),norm=MidpointNormalize(vmin, vmax, 0.))\n",
    "    \n",
    "    ax=fig.axes[0]\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.1,ax.get_position().height])\n",
    "    fig.colorbar(a,cax=cax)\n",
    "    # contour lines\n",
    "    # only draw them when at least one weight is not zero\n",
    "    if len([w for w in weights if w != 0.0]):\n",
    "        contourset = fig.axes[0].contour(\n",
    "            x, y, z, contourLines, colors=colors, alpha=alpha, **kwargs)\n",
    "        for j, c in enumerate(contourset.collections):\n",
    "            if contourset.levels[j] == 0.0:\n",
    "                c.set_linewidth(0.0)\n",
    "            elif contourset.levels[j] < 0:\n",
    "                c.set_dashes([(0, (3.0, 3.0))])\n",
    "    fig.axes[0].set_axis_off()\n",
    "    return fig\n",
    "\n",
    "def plot_features_to_mol(features,mol,title=None,path=None,prefix=\"\",plot=True):    \n",
    "    vmin,vmax=min(0,features.min()),max(1e-6,features.max())\n",
    "    files=[]\n",
    "    for d in range(features.shape[1]):\n",
    "        if path:\n",
    "            filepath=os.path.join(path,\"{}{}.png\".format(prefix,d))\n",
    "            if os.path.exists(filepath) and not REDRAW:\n",
    "                files.append(filepath)\n",
    "                continue\n",
    "        f = customGetSimilarityMapFromWeights(mol,features[:,d],colorMap=\"jet\",vmin=vmin,vmax=vmax)\n",
    "        if title:\n",
    "            f.axes[0].set_title(title[d], fontsize=20)\n",
    "        if path:\n",
    "            files.append(filepath)\n",
    "            plt.savefig(files[-1], bbox_inches = 'tight',dpi=DEFAULT_DPI)\n",
    "\n",
    "        if plot:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.close()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first a blank mol dataset\n",
    "#from molNet.dataloader.datasets import DelaneySolubility\n",
    "#dataset=DelaneySolubility().df\n",
    "from molNet.dataloader.molecule_loader import MoleculeGraphFromDfLoader\n",
    "from molNet.featurizer.atom_featurizer import atom_symbol_one_hot_from_set,atom_hybridization_one_hot\n",
    "import pandas as pd\n",
    "\n",
    "def load_default_df():\n",
    "\n",
    "    dataset = pd.read_csv(\"list_chemicals-2020-12-05-21-46-06.tsv\",sep=\"\\t\")\n",
    "    dataset = dataset.append({\n",
    "        'SMILES':\"C\",\n",
    "        'PREFERRED_NAME':\"Methane\"\n",
    "    },ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    has_fags=dataset.index[dataset[\"SMILES\"].apply(lambda x: \"*\" in x)]\n",
    "    dataset.drop(has_fags,inplace=True)\n",
    "\n",
    "    dataset[\"rd_mol\"] = dataset['SMILES'].apply(lambda s:Chem.MolFromSmiles(s))\n",
    "\n",
    "    no_mols = dataset.index[~dataset[\"rd_mol\"].apply(lambda x: isinstance(x,Chem.Mol))]\n",
    "    dataset.drop(no_mols,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    #dataset[\"rd_mol\"] = dataset['rd_mol'].apply(lambda mol: \n",
    "    #                                           ReplaceSubstructs(ReplaceSubstructs(Chem.AddHs(mol),\n",
    "    #                                                             patt1,repl,replaceAll=True)[0],patt2,repl,replaceAll=True)[0])\n",
    "\n",
    "    dataset[\"SMILES\"] = dataset['rd_mol'].apply(lambda s: Chem.MolToSmiles(s))\n",
    "    dataset[\"molar_mass\"] = dataset['rd_mol'].apply(lambda mol: Chem.Descriptors.MolWt(mol))\n",
    "    dataset = dataset.rename({'PREFERRED_NAME':'name'},axis=1)\n",
    "\n",
    "\n",
    "    _loader = MoleculeGraphFromDfLoader(\n",
    "        dataset,\n",
    "        smiles_column='SMILES',\n",
    "        batch_size=32,\n",
    "        split=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    try:\n",
    "        _loader.train_dataloader()\n",
    "    except:\n",
    "        _loader.setup()\n",
    "    s=0\n",
    "    mol_graphs=[]\n",
    "    for d in _loader.train_dataloader():\n",
    "        mol_graphs.extend(d)\n",
    "\n",
    "    dataset[\"pre_graphs\"]=mol_graphs    \n",
    "\n",
    "    dataset[\"hybridization\"] = dataset['pre_graphs'].apply(lambda mg: np.array([atom_hybridization_one_hot(a) for a in mg.mol.GetAtoms()]))\n",
    "    #dataset[\"hybridization_t\"] = dataset['hybridization'].apply(lambda h: h.T)\n",
    "    #for i,s in enumerate(atom_hybridization_one_hot.describe_features()):\n",
    "    #    dataset[s]=dataset[\"hybridization\"].apply(lambda h:h[:,i])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "#\n",
    "\n",
    "#c_atom_symbol_featurizer.describe_features()\n",
    "#for i,data in dataset.iterrows():\n",
    "#    for atom in data[\"rd_mol\"].GetAtoms():\n",
    "#        if atom.GetSymbol()==\"*\":\n",
    "#            display(data[\"rd_mol\"])\n",
    "#            display(data)\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from rdkit.Chem.rdMolDescriptors import CalcNumRotatableBonds, CalcExactMolWt, CalcNumLipinskiHBD, CalcNumRings, \\\n",
    "    CalcNumLipinskiHBA, CalcNumHBA, CalcNumHBD, CalcNumAromaticRings, CalcNumSaturatedRings, CalcNumHeterocycles, \\\n",
    "    CalcNumAromaticHeterocycles, CalcNumAromaticCarbocycles, CalcNumSaturatedHeterocycles, CalcNumSaturatedCarbocycles, \\\n",
    "    CalcNumAliphaticRings, CalcNumAliphaticHeterocycles, CalcNumAliphaticCarbocycles, CalcNumHeteroatoms, \\\n",
    "    CalcNumAmideBonds, CalcFractionCSP3, CalcLabuteASA, CalcTPSA, CalcChi0v, CalcChi1v, CalcChi2v, CalcChi3v, CalcChi4v, \\\n",
    "    CalcChi0n, CalcChi1n, CalcChi2n, CalcChi3n, CalcChi4n, CalcHallKierAlpha, CalcKappa1, CalcKappa2, CalcKappa3, \\\n",
    "    CalcNumSpiroAtoms, CalcNumBridgeheadAtoms, CalcNumAtomStereoCenters, CalcNumUnspecifiedAtomStereoCenters, CalcPBF, \\\n",
    "    CalcNPR1, CalcNPR2, CalcPMI1, CalcPMI2, CalcPMI3, CalcRadiusOfGyration, CalcInertialShapeFactor, CalcEccentricity, \\\n",
    "    CalcAsphericity, CalcSpherocityIndex, CalcCrippenDescriptors, GetUSR, GetUSRCAT, SlogP_VSA_, SMR_VSA_, PEOE_VSA_, \\\n",
    "    CalcWHIM, CalcGETAWAY, CalcRDF, CalcMORSE, CalcAUTOCORR3D, CalcAUTOCORR2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_test_smiles(dataset):\n",
    "    sdf=dataset.copy()\n",
    "\n",
    "    sdf=sdf[sdf.rd_mol.apply(lambda mol: CalcNumAromaticRings(mol)>0)]\n",
    "    patern=Chem.MolFromSmarts('[N+]([O-])=O')\n",
    "    sdf=sdf[sdf.rd_mol.apply(lambda mol: mol.HasSubstructMatch(patern))]\n",
    "    patern=Chem.MolFromSmarts('cOC')\n",
    "    sdf=sdf[sdf.rd_mol.apply(lambda mol: mol.HasSubstructMatch(patern))]\n",
    "    m =sdf.iloc[0].rd_mol\n",
    "    \n",
    "    return Chem.MolToSmiles(sdf.sort_values(\"molar_mass\",axis=0).rd_mol.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run bg_graphviz.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gallery(images, height='auto',captions=None):\n",
    "    if isinstance (height,(int,float)):\n",
    "        height=str(height)+\"px\"\n",
    "    if not captions:\n",
    "        captions=[None]*len(images)\n",
    "    if len(captions)<len(images):\n",
    "        captions = captions + [None]*(len(captions)-len(images))\n",
    "        \n",
    "    figures = []\n",
    "    for i,image in enumerate(images):\n",
    "        src = image\n",
    "        caption = f'<figcaption>{captions[i]}</figcaption>'\n",
    "        figures.append(f'''\n",
    "            <figure style=\"margin: 5px !important;\">\n",
    "              <img src=\"{src}\" style=\"height: {height}\">\n",
    "              {caption}\n",
    "            </figure>\n",
    "        ''')\n",
    "    return HTML(data=f'''\n",
    "        <div style=\"display: flex; flex-flow: row wrap; text-align: center;\">\n",
    "        {''.join(figures)}\n",
    "        </div>\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molNet.dataloader.molecule_loader import PytorchGeomMolGraphFromGeneratorLoader, PytorchGeomMolGraphGenerator, PytorchGeomMolGraphFromDfLoader\n",
    "import torch.nn.functional as F\n",
    "import molNet.nn.functional as mF\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_false_atom_predictions(loader,model,ignore_subgroups=[]):    \n",
    "    try:\n",
    "        loader.test_dataloader()\n",
    "    except:\n",
    "        loader.setup()\n",
    "    \n",
    "    subgroups=ignore_subgroups.copy()\n",
    "\n",
    "    sgd=[]\n",
    "    for s in subgroups:\n",
    "        ind_map = {}\n",
    "        qmol = Chem.MolFromSmarts(s) \n",
    "        for atom in qmol.GetAtoms() :\n",
    "            map_num = atom.GetAtomMapNum()\n",
    "            if map_num:\n",
    "                ind_map[map_num-1] = atom.GetIdx()\n",
    "        map_list = np.array([ind_map[x] for x in sorted(ind_map)])\n",
    "        sgd.append((qmol,map_list))\n",
    "\n",
    "    for _loader in [loader.test_dataloader(),\n",
    "                    loader.val_dataloader(),\n",
    "                    loader.train_dataloader()]:\n",
    "        for d in _loader:\n",
    "                pred=model(d)\n",
    "                bad_pred=pred.argmax(1)!=d.y.argmax(1)\n",
    "                for batch in d.batch[bad_pred].unique():\n",
    "                    indices=d.batch == batch\n",
    "\n",
    "                    graph=d.mol_graph[batch]\n",
    "                    l_true = short_hybrid[d.y[indices].detach().numpy().argmax(1)].astype(np.object)\n",
    "                    l_pred = short_hybrid[pred[indices].detach().numpy().argmax(1)].astype(np.object)\n",
    "\n",
    "\n",
    "                    wrong_l=l_true!=l_pred\n",
    "\n",
    "                    node_color=np.array(['#1f78b4']*len(graph))\n",
    "                    node_color[wrong_l]=\"red\"\n",
    "                    l=l_true.copy()\n",
    "                    l[wrong_l]=l_pred[wrong_l]+\"(\"+l_true[wrong_l]+\")\"\n",
    "\n",
    "                    mol=graph.molecule.mol\n",
    "                    found=False\n",
    "                    for sg in sgd:\n",
    "                        if found:\n",
    "                            break\n",
    "                        #display(sg[0])\n",
    "                        for match in mol.GetSubstructMatches( sg[0] ):\n",
    "                            match=np.array(match)\n",
    "                            ##print(sg[1])\n",
    "                            mas = match[sg[1]]\n",
    "                            if any(np.where(wrong_l)[0]==mas):\n",
    "                                found=True\n",
    "                                break\n",
    "\n",
    "                    if not found:\n",
    "                        display(graph.molecule)\n",
    "                        f = graph.get_fig(labels=l.tolist(),node_color=node_color)\n",
    "                        plt.show()\n",
    "                        plt.close()\n",
    "                        display(Chem.MolToSmiles(graph.molecule.mol))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChemGCLayer(torch.nn.Module):\n",
    "    def __init__(self,in_features,initlial_net_sizes,gc_out,feats_out,bias=True,linear_activation=None):\n",
    "        super().__init__()\n",
    "        initlial_net_sizes=[in_features]+initlial_net_sizes\n",
    "        inital_net=[]\n",
    "        for i in range(1,len(initlial_net_sizes)):\n",
    "            inital_net.append(\n",
    "                torch.nn.Linear(initlial_net_sizes[i-1], initlial_net_sizes[i],bias=bias)\n",
    "            )\n",
    "            if linear_activation is not None:\n",
    "                inital_net.append(linear_activation)\n",
    "        \n",
    "        self.fcnn=torch.nn.Sequential(*inital_net)\n",
    "        \n",
    "        self.gc = GCNConv(initlial_net_sizes[-1],gc_out,\n",
    "                           bias=bias\n",
    "                          )\n",
    "        \n",
    "        final_net=[torch.nn.Linear(initlial_net_sizes[-1]+gc_out,feats_out,bias=bias)]\n",
    "        if linear_activation is not None:\n",
    "            final_net.append(linear_activation)\n",
    "        self.combine= torch.nn.Sequential(*final_net)\n",
    "        self.feats_out=feats_out\n",
    "     \n",
    "    def forward(self, feats_edges_batch):\n",
    "        feats,edges,batch = feats_edges_batch\n",
    "        nfeats = self.fcnn(feats)\n",
    "        \n",
    "        gc_feats=self.gc(nfeats,edges)\n",
    "         \n",
    "        return self.combine(torch.cat([nfeats,gc_feats], dim=1)),edges,batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molNet",
   "language": "python",
   "name": "molnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}